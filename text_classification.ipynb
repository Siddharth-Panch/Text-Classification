{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "WP4Tx1FSLCZl"
      },
      "id": "WP4Tx1FSLCZl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16254d2c",
      "metadata": {
        "id": "16254d2c"
      },
      "outputs": [],
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing import sequence, text\n",
        "from tensorflow.keras import models, initializers, regularizers\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, SeparableConv1D, MaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/aclImdb.zip"
      ],
      "metadata": {
        "id": "nUtgXn6jMAIl"
      },
      "id": "nUtgXn6jMAIl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "76aa8e9f",
      "metadata": {
        "id": "76aa8e9f"
      },
      "source": [
        "# Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd2771dd",
      "metadata": {
        "id": "dd2771dd"
      },
      "outputs": [],
      "source": [
        "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
        "    \"\"\"Loads the IMDB movie reviews sentiment analysis dataset.\n",
        "        Args:\n",
        "            data_path (str): Path to the data directory.\n",
        "            seed (int): Random seed for reproducibility.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the training and validation datasets. Number of training examples is 25,000 and test examples is 25,000. Number of classes is 2 (0 for negative, 1 for positive).\n",
        "\n",
        "        References:\n",
        "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
        "\n",
        "        Download and uncompress archive from:\n",
        "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    \"\"\"\n",
        "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
        "\n",
        "    # Load training data\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
        "        for fname in sorted(os.listdir(train_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(train_path, fname)) as f:\n",
        "                    train_texts.append(f.read())\n",
        "                train_labels.append(1 if category == 'pos' else 0)\n",
        "\n",
        "    # Load validation data\n",
        "    val_texts = []\n",
        "    val_labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        val_path = os.path.join(imdb_data_path, 'test', category)\n",
        "        for fname in sorted(os.listdir(val_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(val_path, fname)) as f:\n",
        "                    val_texts.append(f.read())\n",
        "                val_labels.append(1 if category == 'pos' else 0)\n",
        "\n",
        "    # Shuffle the training data and labels\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_texts)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_labels)\n",
        "\n",
        "    return ((train_texts, np.array(train_labels)), (val_texts, np.array(val_labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fa420df",
      "metadata": {
        "id": "7fa420df"
      },
      "source": [
        "# Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13853b88",
      "metadata": {
        "id": "13853b88"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = load_imdb_sentiment_analysis_dataset('/content')\n",
        "\n",
        "train_texts, train_labels = train_data\n",
        "val_texts, val_labels = val_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c0176c",
      "metadata": {
        "id": "54c0176c"
      },
      "source": [
        "# Preprocess Data\n",
        "for Technique A using N-Gram Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539eca66",
      "metadata": {
        "id": "539eca66"
      },
      "outputs": [],
      "source": [
        "#Vectorization parameters\n",
        "NGRAM_RANGE = (1, 2)\n",
        "TOP_K = 20000\n",
        "TOKEN_MODE = 'word'\n",
        "MIN_DOCUMENT_FREQUENCY = 2 # Minimum number of documents a term must appear in to be included\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
        "    \"\"\"Vectorizes the training and validation texts using n-grams. 1 text = 1 tf-idf vector the lenght of vocabulary of unigrams + bigrams.\n",
        "\n",
        "    Args:\n",
        "        train_texts (list): List of training texts.\n",
        "        train_labels (np.array): Array of training labels.\n",
        "        val_texts (list): List of validation texts.\n",
        "    Returns:\n",
        "        x_train, x_val (np.array): vectorized training and validation texts.\"\"\"\n",
        "\n",
        "    kwargs = {\n",
        "        'ngram_range': NGRAM_RANGE,\n",
        "        'dtype': 'int32',\n",
        "        'strip_accents': 'unicode',\n",
        "        'decode_error': 'replace',\n",
        "        'analyzer': TOKEN_MODE,\n",
        "        'min_df': MIN_DOCUMENT_FREQUENCY,\n",
        "    }\n",
        "\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "    x_train = vectorizer.fit_transform(train_texts)\n",
        "    x_val = vectorizer.transform(val_texts)\n",
        "\n",
        "    # Select top k features\n",
        "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
        "    selector.fit(x_train, train_labels)\n",
        "    x_train = selector.transform(x_train).astype('float32')\n",
        "    x_val = selector.transform(x_val).astype('float32')\n",
        "    return x_train, x_val"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ef71b1",
      "metadata": {
        "id": "25ef71b1"
      },
      "source": [
        "for Technique B using Seq-Seq Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef9db4da",
      "metadata": {
        "id": "ef9db4da"
      },
      "outputs": [],
      "source": [
        "TOP_K = 20000\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 500 #Sequences longer than this will be truncated.\n",
        "\n",
        "def sequence_vectorize(train_texts, val_texts):\n",
        "    \"\"\"Vectorizes the training and validation texts using sequences. 1 text = 1 sequence vector with fixed length.\n",
        "    Args:\n",
        "        train_texts (list): List of training texts.\n",
        "        val_texts (list): List of validation texts.\n",
        "    Returns:\n",
        "        x_train, x_val, word_index (np.array, np.array, dict): vectorized training and validation texts and word index dictionary.\"\"\"\n",
        "\n",
        "    # Create vocabulary with training texts\n",
        "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "    # Vectorize training and validation texts\n",
        "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
        "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "    # Get max sequence length\n",
        "    max_length = len(max(x_train, key=len))\n",
        "    if max_length > MAX_SEQUENCE_LENGTH:\n",
        "        max_length = MAX_SEQUENCE_LENGTH\n",
        "\n",
        "    # Sequence shorter than max length will be padded in the beginning and sequences longer than max length will be truncated at beginning.\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
        "    return x_train, x_val, tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b19faf9a",
      "metadata": {
        "id": "b19faf9a"
      },
      "source": [
        "# Build Model A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3364978",
      "metadata": {
        "id": "d3364978"
      },
      "outputs": [],
      "source": [
        "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
        "    \"\"\"Creates a multi-layer perceptron model.\n",
        "\n",
        "    Args:\n",
        "        layers (int): Number of 'Dense' layers.\n",
        "        units (int): Output dimension of each 'Dense' layer.\n",
        "        dropout_rate (float): Dropout rate for 'Dropout' layers.\n",
        "        input_shape (tuple): Shape of the input data.\n",
        "        num_classes (int): Number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        model (tensorflow.keras.models.Model): Compiled MLP model.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = (num_classes, 'softmax') if num_classes > 1 else (1, 'sigmoid')\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(op_units, activation=op_activation))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab955ebd",
      "metadata": {
        "id": "ab955ebd"
      },
      "source": [
        "# Build Model B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1654db4f",
      "metadata": {
        "id": "1654db4f"
      },
      "outputs": [],
      "source": [
        "def sepcnn_model(blocks, filters, kernel_size, embedding_dim, dropout_rate, pool_size, input_shape, num_classes, num_features, use_pretrained_embedding=False, is_embedding_trainable=False, embedding_matrix=None):\n",
        "    \"\"\"Creates a separable convolutional neural network model.\n",
        "\n",
        "    Args:\n",
        "        blocks (int): Number of pairs of 'SeparableConv1D' and 'MaxPooling1D' layers.\n",
        "        filters (int): Output dimension of each 'SeparableConv1D' layer.\n",
        "        kernel_size (int): Length of the 1D convolution window.\n",
        "        embedding_dim (int): Dimension of the embedding layer.\n",
        "        dropout_rate (float): Dropout rate for 'Dropout' layers.\n",
        "        pool_size (int): factor by which to downscale the input.\n",
        "        input_shape (tuple): Shape of the input data.\n",
        "        num_classes (int): Number of output classes.\n",
        "        num_features (int): Number of features in the input data.\n",
        "        use_pretrained_embedding (bool): Whether to use a pretrained embedding matrix.\n",
        "        is_embedding_trainable (bool): Whether the embedding layer is trainable.\n",
        "        embedding_matrix (dict): Pretrained embedding matrix.\n",
        "\n",
        "    Returns:\n",
        "        model (tensorflow.keras.models.Model): Compiled separable CNN model.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = (num_classes, 'softmax') if num_classes > 1 else (1, 'sigmoid')\n",
        "    model = models.Sequential()\n",
        "\n",
        "    if use_pretrained_embedding:\n",
        "        model.add(Embedding(input_dim=num_features, output_dim=embedding_dim, input_length=input_shape[0], weights=[embedding_matrix], trainable=is_embedding_trainable))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim=num_features, output_dim=embedding_dim, input_length=input_shape[0]))\n",
        "\n",
        "    for _ in range(blocks-1):\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(SeparableConv1D(filters=filters, kernel_size=kernel_size, activation='relu', bias_initializer='random_uniform', depthwise_initializer='random_uniform', padding='same'))\n",
        "        model.add(SeparableConv1D(filters=filters, kernel_size=kernel_size, activation='relu', bias_initializer='random_uniform', depthwise_initializer='random_uniform', padding='same'))\n",
        "        model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "    model.add(SeparableConv1D(filters=filters*2, kernel_size=kernel_size, activation='relu', bias_initializer='random_uniform', depthwise_initializer='random_uniform', padding='same'))\n",
        "    model.add(SeparableConv1D(filters=filters*2, kernel_size=kernel_size, activation='relu', bias_initializer='random_uniform', depthwise_initializer='random_uniform', padding='same'))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(op_units, activation=op_activation))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62377c74",
      "metadata": {
        "id": "62377c74"
      },
      "source": [
        "# Training Model A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc5f123",
      "metadata": {
        "id": "4fc5f123"
      },
      "outputs": [],
      "source": [
        "def train_ngram_model(data, learning_rate=0.001, epochs=1000, batch_size=128, layers=2, units=64, dropout_rate=0.2):\n",
        "    \"\"\"Trains a multi-layer perceptron model on n-gram vectorized data.\n",
        "\n",
        "    Args:\n",
        "        data (tuple): Tuple containing training and validation data.\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "        epochs (int): Number of epochs to train the model.\n",
        "        batch_size (int): Batch size for training.\n",
        "        layers (int): Number of 'Dense' layers in the model.\n",
        "        units (int): Output dimension of each 'Dense' layer.\n",
        "        dropout_rate (float): Dropout rate for 'Dropout' layers.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If validation data has label values that are not in the training data.\n",
        "    \"\"\"\n",
        "    train_data, val_data = data\n",
        "    train_texts, train_labels = train_data\n",
        "    val_texts, val_labels = val_data\n",
        "\n",
        "    num_classes = 2\n",
        "\n",
        "    unexpected_labels = set(val_labels) - set(train_labels)\n",
        "\n",
        "    if unexpected_labels:\n",
        "        raise ValueError(f\"Validation data has label values that are not in the training data: {unexpected_labels}\")\n",
        "\n",
        "    x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)\n",
        "\n",
        "    x_train = x_train.toarray()\n",
        "    x_val = x_val.toarray()\n",
        "\n",
        "    model = mlp_model(layers=layers, units=units, dropout_rate=dropout_rate, input_shape=(x_train.shape[1],), num_classes=num_classes)\n",
        "\n",
        "    loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "    optimizer = tensorflow.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # Create callback for early stopping on validation loss. If the loss does not decrease in two consecutuve tries, stop training.\n",
        "    callbacks = [\n",
        "        tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "    history = model.fit(x_train, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(x_val, val_labels), callbacks=callbacks, verbose=2)\n",
        "\n",
        "    # Print results\n",
        "    history = history.history\n",
        "    print(f\"Validation accuracy: {history['val_accuracy'][-1]:.4f}, Validation loss: {history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save('ngram_model.h5')\n",
        "    return history['val_accuracy'][-1], history['val_loss'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00bef2c7",
      "metadata": {
        "id": "00bef2c7"
      },
      "outputs": [],
      "source": [
        "train_ngram_model(data=(train_data, val_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f895018",
      "metadata": {
        "id": "3f895018"
      },
      "source": [
        "# Training Model B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cdb8510",
      "metadata": {
        "id": "7cdb8510"
      },
      "outputs": [],
      "source": [
        "def train_sequence_model(data, learning_rate=0.001, epochs=1000, batch_size=128, blocks=2, filters=64, kernel_size=3, embedding_dim=200, dropout_rate=0.2, pool_size=3):\n",
        "    \"\"\"Trains a separable convolutional neural network model on sequence vectorized data.\n",
        "\n",
        "    Args:\n",
        "        data (tuple): Tuple containing training and validation data.\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "        epochs (int): Number of epochs to train the model.\n",
        "        batch_size (int): Batch size for training.\n",
        "        blocks (int): Number of pairs of 'SeparableConv1D' and 'MaxPooling1D' layers in the model.\n",
        "        filters (int): Output dimension of each 'SeparableConv1D' layer.\n",
        "        kernel_size (int): Length of the 1D convolution window.\n",
        "        embedding_dim (int): Dimension of the embedding layer.\n",
        "        dropout_rate (float): Dropout rate for 'Dropout' layers.\n",
        "        pool_size (int): Factor by which to downscale the input.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If validation data has label values that are not in the training data.\n",
        "    \"\"\"\n",
        "    train_data, val_data = data\n",
        "    train_texts, train_labels = train_data\n",
        "    val_texts, val_labels = val_data\n",
        "\n",
        "    num_classes = 2\n",
        "\n",
        "    unexpected_labels = set(val_labels) - set(train_labels)\n",
        "\n",
        "    if unexpected_labels:\n",
        "        raise ValueError(f\"Validation data has label values that are not in the training data: {unexpected_labels}\")\n",
        "\n",
        "    x_train, x_val, word_index = sequence_vectorize(train_texts, val_texts)\n",
        "\n",
        "    num_features = min(len(word_index) + 1, TOP_K)\n",
        "\n",
        "    model = sepcnn_model(blocks=blocks, filters=filters, kernel_size=kernel_size, embedding_dim=embedding_dim, dropout_rate=dropout_rate, pool_size=pool_size, input_shape=(x_train.shape[1], 1), num_classes=num_classes, num_features=num_features)\n",
        "\n",
        "    loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "    optimizer = tensorflow.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # Create callback for early stopping on validation loss. If the loss does not decrease in two consecutuve tries, stop training.\n",
        "    callbacks = [\n",
        "        tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "    history = model.fit(x_train, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(x_val, val_labels), callbacks=callbacks, verbose=2)\n",
        "\n",
        "    # Print results\n",
        "    history = history.history\n",
        "    print(f\"Validation accuracy: {history['val_accuracy'][-1]:.4f}, Validation loss: {history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save('sequence_model.h5')\n",
        "    return history['val_accuracy'][-1], history['val_loss'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3edbad3f",
      "metadata": {
        "id": "3edbad3f"
      },
      "outputs": [],
      "source": [
        "train_sequence_model(data=(train_data, val_data))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}