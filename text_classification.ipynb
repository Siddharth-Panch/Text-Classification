{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16254d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "from explore_data import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "from tensorflow.keras import models, initializers, regularizers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, SeparableConv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa8e9f",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2771dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
    "    \"\"\"Loads the IMDB movie reviews sentiment analysis dataset.\n",
    "        Args:\n",
    "            data_path (str): Path to the data directory.\n",
    "            seed (int): Random seed for reproducibility.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple containing the training and validation datasets. Number of training examples is 25,000 and test examples is 25,000. Number of classes is 2 (0 for negative, 1 for positive).\n",
    "        \n",
    "        References:\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(1 if category == 'pos' else 0)\n",
    "\n",
    "    # Load validation data\n",
    "    val_texts = []\n",
    "    val_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        val_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(val_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(val_path, fname)) as f:\n",
    "                    val_texts.append(f.read())\n",
    "                val_labels.append(1 if category == 'pos' else 0)\n",
    "\n",
    "    # Shuffle the training data and labels\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)), (val_texts, np.array(val_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa420df",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13853b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = load_imdb_sentiment_analysis_dataset('/home/siddharth/Desktop/Text Classification')\n",
    "\n",
    "train_texts, train_labels = train_data\n",
    "val_texts, val_labels = val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of classes in training data: {get_num_classes(train_labels)}\")\n",
    "print(f\"Number of classes in validation data: {get_num_classes(val_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02af457",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of words per sample in training data: {get_num_words_per_sample(train_texts)}\")\n",
    "print(f\"Number of words per sample in validation data: {get_num_words_per_sample(val_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e65fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequency_distribution_of_ngrams(train_texts, num_ngrams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3feb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequency_distribution_of_ngrams(val_texts, num_ngrams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_length_distribution(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_length_distribution(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef62dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c0176c",
   "metadata": {},
   "source": [
    "# Preprocess Data \n",
    "for Technique A using N-Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539eca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization parameters\n",
    "NGRAM_RANGE = (1, 2)  \n",
    "TOP_K = 20000\n",
    "TOKEN_MODE = 'word'\n",
    "MIN_DOCUMENT_FREQUENCY = 2 # Minimum number of documents a term must appear in to be included\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes the training and validation texts using n-grams. 1 text = 1 tf-idf vector the lenght of vocabulary of unigrams + bigrams.\n",
    "    \n",
    "    Args:\n",
    "        train_texts (list): List of training texts.\n",
    "        train_labels (np.array): Array of training labels.\n",
    "        val_texts (list): List of validation texts.\n",
    "    Returns:\n",
    "        x_train, x_val (np.array): vectorized training and validation texts.\"\"\"\n",
    "    \n",
    "    kwargs = {\n",
    "        'ngram_range': NGRAM_RANGE,\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,\n",
    "        'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top k features\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef71b1",
   "metadata": {},
   "source": [
    "for Technique B using Seq-Seq Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9db4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 20000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 500 #Sequences longer than this will be truncated.\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes the training and validation texts using sequences. 1 text = 1 sequence vector with fixed length.\n",
    "    Args:\n",
    "        train_texts (list): List of training texts.\n",
    "        val_texts (list): List of validation texts.\n",
    "    Returns:\n",
    "        x_train, x_val, word_index (np.array, np.array, dict): vectorized training and validation texts and word index dictionary.\"\"\" \n",
    "    \n",
    "    # Create vocabulary with training texts\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length\n",
    "    max_length = len(max(x_train, key=len)) \n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Sequence shorter than max length will be padded in the beginning and sequences longer than max length will be truncated at beginning.    \n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19faf9a",
   "metadata": {},
   "source": [
    "# Build Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3364978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates a multi-layer perceptron model.\n",
    "    \n",
    "    Args:\n",
    "        layers (int): Number of 'Dense' layers.\n",
    "        units (int): Output dimension of each 'Dense' layer.\n",
    "        dropout_rate (float): Dropout rate for 'Dropout' layers.\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "        num_classes (int): Number of output classes.\n",
    "        \n",
    "    Returns:\n",
    "        model (tensorflow.keras.models.Model): Compiled MLP model.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = (num_classes, 'softmax') if num_classes > 1 else (1, 'sigmoid')\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab955ebd",
   "metadata": {},
   "source": [
    "# Build Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1654db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepcnn_model(blocks, filters, kernel_size, embedding_dim, dropout_rate, pool_size, input_shape, num_classes, num_features, use_pretrained_embedding=False, is_embedding_trainable=False, embedding_matrix=None):\n",
    "    \"\"\"Creates a separable convolutional neural network model.\n",
    "\n",
    "    Args:\n",
    "        blocks (int): Number of pairs of 'SeparableConv1D' and 'MaxPooling1D' layers.\n",
    "        filters (int): Output dimension of each 'SeparableConv1D' layer.\n",
    "        kernel_size (int): Length of the 1D convolution window.\n",
    "        embedding_dim (int): Dimension of the embedding layer.\n",
    "        dropout_rate (float): Dropout rate for 'Dropout' layers.\n",
    "        pool_size (int): factor by which to downscale the input.\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "        num_classes (int): Number of output classes.\n",
    "        num_features (int): Number of features in the input data.\n",
    "        use_pretrained_embedding (bool): Whether to use a pretrained embedding matrix.\n",
    "        is_embedding_trainable (bool): Whether the embedding layer is trainable.\n",
    "        embedding_matrix (dict): Pretrained embedding matrix.\n",
    "    \n",
    "    Returns:\n",
    "        model (tensorflow.keras.models.Model): Compiled separable CNN model.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = (num_classes, 'softmax') if num_classes > 1 else (1, 'sigmoid')\n",
    "    model = models.Sequential()\n",
    "\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features, output_dim=embedding_dim, input_length=input_shape[0], weights=[embedding_matrix], trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features, output_dim=embedding_dim, input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(SeparableConv1D(filters=filters, kernel_size=kernel_size, activation='relu', bias_initializer='random_uniform', depthwise_initializer='random_uniform', padding='same'))\n",
    "        model.add(SeparableConv1D(filters=filters, kernel_size=kernel_size, activation='relu', bias_initializer='random_uniform', depthwise_initializer='random_uniform', padding='same')) \n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters*2, kernel_size=kernel_size, activation='relu', bias_initializer='random_uniform', depthwise_initializer='random_uniform', padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters*2, kernel_size=kernel_size, activation='relu', bias_initializer='random_uniform', depthwise_initializer='random_uniform', padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62377c74",
   "metadata": {},
   "source": [
    "# Training Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_model(data, learning_rate=0.001, epochs=1000, batch_size=128, layers=2, units=64, dropout_rate=0.2):\n",
    "    \"\"\"Trains a multi-layer perceptron model on n-gram vectorized data.\n",
    "\n",
    "    Args:\n",
    "        data (tuple): Tuple containing training and validation data.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        epochs (int): Number of epochs to train the model.\n",
    "        batch_size (int): Batch size for training.\n",
    "        layers (int): Number of 'Dense' layers in the model.\n",
    "        units (int): Output dimension of each 'Dense' layer.\n",
    "        dropout_rate (float): Dropout rate for 'Dropout' layers.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If validation data has label values that are not in the training data.\n",
    "    \"\"\"\n",
    "    train_data, val_data = data\n",
    "    train_texts, train_labels = train_data\n",
    "    val_texts, val_labels = val_data\n",
    "\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "\n",
    "    unexpected_labels = set(val_labels) - set(train_labels)\n",
    "\n",
    "    if unexpected_labels:\n",
    "        raise ValueError(f\"Validation data has label values that are not in the training data: {unexpected_labels}\")\n",
    "    \n",
    "    x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)\n",
    "    \n",
    "    model = mlp_model(layers=layers, units=units, dropout_rate=dropout_rate, input_shape=(x_train.shape[1],), num_classes=num_classes)\n",
    "\n",
    "    loss = 'sparse_categorical_crossentropy'\n",
    "\n",
    "    optimizer = tensorflow.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does not decrease in two consecutuve tries, stop training.\n",
    "    callbacks = [\n",
    "        tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "    \n",
    "    print(\"x_train\", x_train.shape)\n",
    "    print(\"x_val\", x_val.shape)\n",
    "    print(\"train_labels\", train_labels)\n",
    "    print(\"val_labels\", val_labels.shape)\n",
    "    history = model.fit(x_train, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(x_val, val_labels), callbacks=callbacks, verbose=2)\n",
    "\n",
    "    # Print results\n",
    "    history = history.history\n",
    "    print(f\"Validation accuracy: {history['val_accuracy'][-1]:.4f}, Validation loss: {history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    model.save('ngram_model.h5')\n",
    "    return history['val_accuracy'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bef2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngram_model(data=(train_data, val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f895018",
   "metadata": {},
   "source": [
    "# Training Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sequence_model(data, learning_rate=0.001, epochs=1000, batch_size=128, blocks=2, filters=64, kernel_size=3, embedding_dim=200, dropout_rate=0.2, pool_size=3):\n",
    "    \"\"\"Trains a separable convolutional neural network model on sequence vectorized data.\n",
    "\n",
    "    Args:\n",
    "        data (tuple): Tuple containing training and validation data.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        epochs (int): Number of epochs to train the model.\n",
    "        batch_size (int): Batch size for training.\n",
    "        blocks (int): Number of pairs of 'SeparableConv1D' and 'MaxPooling1D' layers in the model.\n",
    "        filters (int): Output dimension of each 'SeparableConv1D' layer.\n",
    "        kernel_size (int): Length of the 1D convolution window.\n",
    "        embedding_dim (int): Dimension of the embedding layer.\n",
    "        dropout_rate (float): Dropout rate for 'Dropout' layers.\n",
    "        pool_size (int): Factor by which to downscale the input.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If validation data has label values that are not in the training data.\n",
    "    \"\"\"\n",
    "    train_data, val_data = data\n",
    "    train_texts, train_labels = train_data\n",
    "    val_texts, val_labels = val_data\n",
    "\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "\n",
    "    unexpected_labels = set(val_labels) - set(train_labels)\n",
    "\n",
    "    if unexpected_labels:\n",
    "        raise ValueError(f\"Validation data has label values that are not in the training data: {unexpected_labels}\")\n",
    "    \n",
    "    x_train, x_val, word_index = sequence_vectorize(train_texts, val_texts)\n",
    "\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    model = sepcnn_model(blocks=blocks, filters=filters, kernel_size=kernel_size, embedding_dim=embedding_dim, dropout_rate=dropout_rate, pool_size=pool_size, input_shape=(x_train.shape[1], 1), num_classes=num_classes, num_features=num_features)\n",
    "\n",
    "    loss = 'sparse_categorical_crossentropy'\n",
    "\n",
    "    optimizer = tensorflow.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does not decrease in two consecutuve tries, stop training.\n",
    "    callbacks = [\n",
    "        tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "    \n",
    "    history = model.fit(x_train, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(x_val, val_labels), callbacks=callbacks, verbose=2)\n",
    "\n",
    "    # Print results\n",
    "    history = history.history\n",
    "    print(f\"Validation accuracy: {history['val_accuracy'][-1]:.4f}, Validation loss: {history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    model.save('sequence_model.h5')\n",
    "    return history['val_accuracy'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence_model(data=(train_data, val_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
